{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "# inside these folders there are multiple text files\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "texts = [] # texts from all the text files\n",
    "class_labels = [] # labels\n",
    "\n",
    "for c in classes:\n",
    "    for f in glob.glob(os.path.join('bbc', c, '*.txt')):\n",
    "        texts.append(open(f).read())\n",
    "        class_labels.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "2225\n",
      "Ad sales boost Time Warner profit\n",
      "\n",
      "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\n",
      "\n",
      "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
      "\n",
      "Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n",
      "\n",
      "Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n",
      "\n",
      "TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
      "\n",
      "business\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(len(class_labels))\n",
    "print(texts[0])\n",
    "print(class_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have 5 classes ['business', 'entertainment', 'politics', 'sport', 'tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Seed and Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "random.seed(10)\n",
    "numpy.random.seed(10)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "z = list(zip(texts, class_labels))\n",
    "random.shuffle(z)\n",
    "\n",
    "texts, class_labels = [a[0] for a in z], [a[1] for a in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sport',\n",
       " 'entertainment',\n",
       " 'sport',\n",
       " 'business',\n",
       " 'sport',\n",
       " 'politics',\n",
       " 'business',\n",
       " 'sport',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'business',\n",
       " 'entertainment',\n",
       " 'politics',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'entertainment',\n",
       " 'business',\n",
       " 'tech',\n",
       " 'sport',\n",
       " 'business']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_length = 512, padding_side = 'right')\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(texts[0], add_special_tokens=True, max_length = 512, pad_to_max_length = True)).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2829,  7879, 22889,  2819,  2006,  3088,  4440,  7306,  5146,\n",
      "          2829,  2038,  4716,  7938,  1005,  1055,  5221, 17137,  3723,  4665,\n",
      "          2004,  2002,  2441,  1037,  2733,  1011,  2146,  3942,  2000,  3088,\n",
      "          1012,  2720,  2829,  1005,  1055,  4440,  2003,  2881,  2000, 12944,\n",
      "          2129,  1996,  2866,  4122,  2000,  2191,  3088,  1005,  1055,  3471,\n",
      "          1037,  9470,  1997,  2049,  3472,  9650,  1997,  1996,  1043,  2620,\n",
      "          2023,  2095,  1012,  2002,  2097,  2156,  2019,  9820,  1013,  8387,\n",
      "         18504,  1999, 11959,  1998,  1037,  2308,  1005,  1055,  4923,  2586,\n",
      "          1999, 16274,  2077,  3242,  2075,  1037,  3116,  1997,  1996,  3222,\n",
      "          2005,  3088,  1999,  4880,  2237,  1012,  2012, 22889, 18163,  1999,\n",
      "          6583,  3217,  5638,  2006,  9317,  1010,  2002,  2056,  2495,  3791,\n",
      "          2018,  2000,  2022, 26176,  1012,  4092,  2648,  1996,  4386,  3078,\n",
      "          2082,  1010,  2720,  2829,  2056,  1024,  1000,  2009,  2003,  3432,\n",
      "          2025, 11701,  1999,  1996,  2715,  2287,  2005,  1996,  2717,  1997,\n",
      "          1996,  2088,  2000,  3233,  2011,  1998,  2031,  5606,  1997,  8817,\n",
      "          1997,  2336,  2025,  2893,  1996,  3382,  2012,  2495,  1012,  1000,\n",
      "          2002,  4197,  2000,  2248,  3488,  2000, 15697,  1002,  2184, 24700,\n",
      "          2005,  2495,  1999,  3088,  2058,  1996,  2279,  5476,  1012,  1996,\n",
      "          2082,  2003,  2006,  1996,  3341,  1997, 11382,  5677,  2050,  1010,\n",
      "          2073,  5385,  1010,  2199,  2444,  2411,  1999, 23326,  2081,  1997,\n",
      "          8494,  1010, 15121,  3384,  1998, 19747,  1012,  2720,  2829,  1005,\n",
      "          1055, 14895,  2015,  2360,  2002,  4122,  2000,  2424,  2041,  2062,\n",
      "          2055,  1996, 20428,  2231,  1005,  1055,  2495,  6043,  1010,  2029,\n",
      "          2443, 10449,  2489,  3078,  2495,  1999,  2220,  2494,  1012,  1996,\n",
      "          7306,  2038,  2525, 11521, 10340,  2005,  1037,  1043,  2620,  4681,\n",
      "          7427,  2029,  2002,  2038, 28834,  2000,  1996,  5832,  2933,  2109,\n",
      "          2011,  1996,  2142,  2163,  2000, 14591,  2885,  2044,  2088,  2162,\n",
      "          2048,  1012,  1996,  4440,  4076,  4447,  1997,  1999, 22158,  2090,\n",
      "          2720,  2829,  1998,  4116, 10503,  6851,  1999,  1037,  2047,  2338,\n",
      "          1012,  4603,  3003,  2745,  4922,  2003,  3517,  2000, 15126,  2006,\n",
      "          2216,  4311,  2012,  3539,  2704,  1005,  1055,  3980,  2012, 14840,\n",
      "         13938,  2102,  2006,  9317,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need attention masks\n",
    "\n",
    "encoded = tokenizer.encode_plus(texts[0], add_special_tokens=True, max_length = 512, pad_to_max_length = True,\n",
    "                                return_token_type_ids = False,\n",
    "                                return_attention_mask = True)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained BERT for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5) # as we have 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last linear layer has 5 out_features. `(classifier): Linear(in_features=768, out_features=5, bias=True)`\n",
    "\n",
    "We can just add a softmax after that and do our regular classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, li = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5, output_loading_info=True) # as we have 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'missing_keys': ['classifier.weight', 'classifier.bias'], 'unexpected_keys': ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'], 'error_msgs': []}\n"
     ]
    }
   ],
   "source": [
    "print(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Bert Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5)\n",
    "        # as we have 5 classes\n",
    "        \n",
    "        # we want our output as probability so, in the evaluation mode, we'll pass the logits to a softmax layer\n",
    "        self.softmax = torch.nn.Softmax(dim = 1) # last dimension\n",
    "    def forward(self, x, attn_mask = None, labels = None):\n",
    "        \n",
    "        if self.training == True:\n",
    "            # print(x.shape)\n",
    "            loss = self.bert(x, attention_mask = attn_mask, labels = labels)\n",
    "            # print(x[0].shape)\n",
    "            \n",
    "            return loss\n",
    "\n",
    "        if self.training == False: # in evaluation mode\n",
    "            x = self.bert(x)\n",
    "            x = self.softmax(x[0])\n",
    "            \n",
    "            return x\n",
    "    def freeze_layers(self, last_trainable = 1): \n",
    "        # we freeze all the layers except the last classification layer + few transformer blocks\n",
    "        for layer in list(self.bert.parameters())[:-last_trainable]:\n",
    "            layer.requires_grad = False\n",
    "        \n",
    "    \n",
    "# create our model\n",
    "\n",
    "bertclassifier = BertClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bert.bert.embeddings.word_embeddings.weight\n",
      "1 bert.bert.embeddings.position_embeddings.weight\n",
      "2 bert.bert.embeddings.token_type_embeddings.weight\n",
      "3 bert.bert.embeddings.LayerNorm.weight\n",
      "4 bert.bert.embeddings.LayerNorm.bias\n",
      "5 bert.bert.encoder.layer.0.attention.self.query.weight\n",
      "6 bert.bert.encoder.layer.0.attention.self.query.bias\n",
      "7 bert.bert.encoder.layer.0.attention.self.key.weight\n",
      "8 bert.bert.encoder.layer.0.attention.self.key.bias\n",
      "9 bert.bert.encoder.layer.0.attention.self.value.weight\n",
      "10 bert.bert.encoder.layer.0.attention.self.value.bias\n",
      "11 bert.bert.encoder.layer.0.attention.output.dense.weight\n",
      "12 bert.bert.encoder.layer.0.attention.output.dense.bias\n",
      "13 bert.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "14 bert.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "15 bert.bert.encoder.layer.0.intermediate.dense.weight\n",
      "16 bert.bert.encoder.layer.0.intermediate.dense.bias\n",
      "17 bert.bert.encoder.layer.0.output.dense.weight\n",
      "18 bert.bert.encoder.layer.0.output.dense.bias\n",
      "19 bert.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "20 bert.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "21 bert.bert.encoder.layer.1.attention.self.query.weight\n",
      "22 bert.bert.encoder.layer.1.attention.self.query.bias\n",
      "23 bert.bert.encoder.layer.1.attention.self.key.weight\n",
      "24 bert.bert.encoder.layer.1.attention.self.key.bias\n",
      "25 bert.bert.encoder.layer.1.attention.self.value.weight\n",
      "26 bert.bert.encoder.layer.1.attention.self.value.bias\n",
      "27 bert.bert.encoder.layer.1.attention.output.dense.weight\n",
      "28 bert.bert.encoder.layer.1.attention.output.dense.bias\n",
      "29 bert.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "30 bert.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "31 bert.bert.encoder.layer.1.intermediate.dense.weight\n",
      "32 bert.bert.encoder.layer.1.intermediate.dense.bias\n",
      "33 bert.bert.encoder.layer.1.output.dense.weight\n",
      "34 bert.bert.encoder.layer.1.output.dense.bias\n",
      "35 bert.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "36 bert.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "37 bert.bert.encoder.layer.2.attention.self.query.weight\n",
      "38 bert.bert.encoder.layer.2.attention.self.query.bias\n",
      "39 bert.bert.encoder.layer.2.attention.self.key.weight\n",
      "40 bert.bert.encoder.layer.2.attention.self.key.bias\n",
      "41 bert.bert.encoder.layer.2.attention.self.value.weight\n",
      "42 bert.bert.encoder.layer.2.attention.self.value.bias\n",
      "43 bert.bert.encoder.layer.2.attention.output.dense.weight\n",
      "44 bert.bert.encoder.layer.2.attention.output.dense.bias\n",
      "45 bert.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "46 bert.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "47 bert.bert.encoder.layer.2.intermediate.dense.weight\n",
      "48 bert.bert.encoder.layer.2.intermediate.dense.bias\n",
      "49 bert.bert.encoder.layer.2.output.dense.weight\n",
      "50 bert.bert.encoder.layer.2.output.dense.bias\n",
      "51 bert.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "52 bert.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "53 bert.bert.encoder.layer.3.attention.self.query.weight\n",
      "54 bert.bert.encoder.layer.3.attention.self.query.bias\n",
      "55 bert.bert.encoder.layer.3.attention.self.key.weight\n",
      "56 bert.bert.encoder.layer.3.attention.self.key.bias\n",
      "57 bert.bert.encoder.layer.3.attention.self.value.weight\n",
      "58 bert.bert.encoder.layer.3.attention.self.value.bias\n",
      "59 bert.bert.encoder.layer.3.attention.output.dense.weight\n",
      "60 bert.bert.encoder.layer.3.attention.output.dense.bias\n",
      "61 bert.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "62 bert.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "63 bert.bert.encoder.layer.3.intermediate.dense.weight\n",
      "64 bert.bert.encoder.layer.3.intermediate.dense.bias\n",
      "65 bert.bert.encoder.layer.3.output.dense.weight\n",
      "66 bert.bert.encoder.layer.3.output.dense.bias\n",
      "67 bert.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "68 bert.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "69 bert.bert.encoder.layer.4.attention.self.query.weight\n",
      "70 bert.bert.encoder.layer.4.attention.self.query.bias\n",
      "71 bert.bert.encoder.layer.4.attention.self.key.weight\n",
      "72 bert.bert.encoder.layer.4.attention.self.key.bias\n",
      "73 bert.bert.encoder.layer.4.attention.self.value.weight\n",
      "74 bert.bert.encoder.layer.4.attention.self.value.bias\n",
      "75 bert.bert.encoder.layer.4.attention.output.dense.weight\n",
      "76 bert.bert.encoder.layer.4.attention.output.dense.bias\n",
      "77 bert.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "78 bert.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "79 bert.bert.encoder.layer.4.intermediate.dense.weight\n",
      "80 bert.bert.encoder.layer.4.intermediate.dense.bias\n",
      "81 bert.bert.encoder.layer.4.output.dense.weight\n",
      "82 bert.bert.encoder.layer.4.output.dense.bias\n",
      "83 bert.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "84 bert.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "85 bert.bert.encoder.layer.5.attention.self.query.weight\n",
      "86 bert.bert.encoder.layer.5.attention.self.query.bias\n",
      "87 bert.bert.encoder.layer.5.attention.self.key.weight\n",
      "88 bert.bert.encoder.layer.5.attention.self.key.bias\n",
      "89 bert.bert.encoder.layer.5.attention.self.value.weight\n",
      "90 bert.bert.encoder.layer.5.attention.self.value.bias\n",
      "91 bert.bert.encoder.layer.5.attention.output.dense.weight\n",
      "92 bert.bert.encoder.layer.5.attention.output.dense.bias\n",
      "93 bert.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "94 bert.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "95 bert.bert.encoder.layer.5.intermediate.dense.weight\n",
      "96 bert.bert.encoder.layer.5.intermediate.dense.bias\n",
      "97 bert.bert.encoder.layer.5.output.dense.weight\n",
      "98 bert.bert.encoder.layer.5.output.dense.bias\n",
      "99 bert.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "100 bert.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "101 bert.bert.encoder.layer.6.attention.self.query.weight\n",
      "102 bert.bert.encoder.layer.6.attention.self.query.bias\n",
      "103 bert.bert.encoder.layer.6.attention.self.key.weight\n",
      "104 bert.bert.encoder.layer.6.attention.self.key.bias\n",
      "105 bert.bert.encoder.layer.6.attention.self.value.weight\n",
      "106 bert.bert.encoder.layer.6.attention.self.value.bias\n",
      "107 bert.bert.encoder.layer.6.attention.output.dense.weight\n",
      "108 bert.bert.encoder.layer.6.attention.output.dense.bias\n",
      "109 bert.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "110 bert.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "111 bert.bert.encoder.layer.6.intermediate.dense.weight\n",
      "112 bert.bert.encoder.layer.6.intermediate.dense.bias\n",
      "113 bert.bert.encoder.layer.6.output.dense.weight\n",
      "114 bert.bert.encoder.layer.6.output.dense.bias\n",
      "115 bert.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "116 bert.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "117 bert.bert.encoder.layer.7.attention.self.query.weight\n",
      "118 bert.bert.encoder.layer.7.attention.self.query.bias\n",
      "119 bert.bert.encoder.layer.7.attention.self.key.weight\n",
      "120 bert.bert.encoder.layer.7.attention.self.key.bias\n",
      "121 bert.bert.encoder.layer.7.attention.self.value.weight\n",
      "122 bert.bert.encoder.layer.7.attention.self.value.bias\n",
      "123 bert.bert.encoder.layer.7.attention.output.dense.weight\n",
      "124 bert.bert.encoder.layer.7.attention.output.dense.bias\n",
      "125 bert.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "126 bert.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "127 bert.bert.encoder.layer.7.intermediate.dense.weight\n",
      "128 bert.bert.encoder.layer.7.intermediate.dense.bias\n",
      "129 bert.bert.encoder.layer.7.output.dense.weight\n",
      "130 bert.bert.encoder.layer.7.output.dense.bias\n",
      "131 bert.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "132 bert.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "133 bert.bert.encoder.layer.8.attention.self.query.weight\n",
      "134 bert.bert.encoder.layer.8.attention.self.query.bias\n",
      "135 bert.bert.encoder.layer.8.attention.self.key.weight\n",
      "136 bert.bert.encoder.layer.8.attention.self.key.bias\n",
      "137 bert.bert.encoder.layer.8.attention.self.value.weight\n",
      "138 bert.bert.encoder.layer.8.attention.self.value.bias\n",
      "139 bert.bert.encoder.layer.8.attention.output.dense.weight\n",
      "140 bert.bert.encoder.layer.8.attention.output.dense.bias\n",
      "141 bert.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "142 bert.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "143 bert.bert.encoder.layer.8.intermediate.dense.weight\n",
      "144 bert.bert.encoder.layer.8.intermediate.dense.bias\n",
      "145 bert.bert.encoder.layer.8.output.dense.weight\n",
      "146 bert.bert.encoder.layer.8.output.dense.bias\n",
      "147 bert.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "148 bert.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "149 bert.bert.encoder.layer.9.attention.self.query.weight\n",
      "150 bert.bert.encoder.layer.9.attention.self.query.bias\n",
      "151 bert.bert.encoder.layer.9.attention.self.key.weight\n",
      "152 bert.bert.encoder.layer.9.attention.self.key.bias\n",
      "153 bert.bert.encoder.layer.9.attention.self.value.weight\n",
      "154 bert.bert.encoder.layer.9.attention.self.value.bias\n",
      "155 bert.bert.encoder.layer.9.attention.output.dense.weight\n",
      "156 bert.bert.encoder.layer.9.attention.output.dense.bias\n",
      "157 bert.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "158 bert.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "159 bert.bert.encoder.layer.9.intermediate.dense.weight\n",
      "160 bert.bert.encoder.layer.9.intermediate.dense.bias\n",
      "161 bert.bert.encoder.layer.9.output.dense.weight\n",
      "162 bert.bert.encoder.layer.9.output.dense.bias\n",
      "163 bert.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "164 bert.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "165 bert.bert.encoder.layer.10.attention.self.query.weight\n",
      "166 bert.bert.encoder.layer.10.attention.self.query.bias\n",
      "167 bert.bert.encoder.layer.10.attention.self.key.weight\n",
      "168 bert.bert.encoder.layer.10.attention.self.key.bias\n",
      "169 bert.bert.encoder.layer.10.attention.self.value.weight\n",
      "170 bert.bert.encoder.layer.10.attention.self.value.bias\n",
      "171 bert.bert.encoder.layer.10.attention.output.dense.weight\n",
      "172 bert.bert.encoder.layer.10.attention.output.dense.bias\n",
      "173 bert.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "174 bert.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "175 bert.bert.encoder.layer.10.intermediate.dense.weight\n",
      "176 bert.bert.encoder.layer.10.intermediate.dense.bias\n",
      "177 bert.bert.encoder.layer.10.output.dense.weight\n",
      "178 bert.bert.encoder.layer.10.output.dense.bias\n",
      "179 bert.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "180 bert.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "181 bert.bert.encoder.layer.11.attention.self.query.weight\n",
      "182 bert.bert.encoder.layer.11.attention.self.query.bias\n",
      "183 bert.bert.encoder.layer.11.attention.self.key.weight\n",
      "184 bert.bert.encoder.layer.11.attention.self.key.bias\n",
      "185 bert.bert.encoder.layer.11.attention.self.value.weight\n",
      "186 bert.bert.encoder.layer.11.attention.self.value.bias\n",
      "187 bert.bert.encoder.layer.11.attention.output.dense.weight\n",
      "188 bert.bert.encoder.layer.11.attention.output.dense.bias\n",
      "189 bert.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "190 bert.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "191 bert.bert.encoder.layer.11.intermediate.dense.weight\n",
      "192 bert.bert.encoder.layer.11.intermediate.dense.bias\n",
      "193 bert.bert.encoder.layer.11.output.dense.weight\n",
      "194 bert.bert.encoder.layer.11.output.dense.bias\n",
      "195 bert.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "196 bert.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "197 bert.bert.pooler.dense.weight\n",
      "198 bert.bert.pooler.dense.bias\n",
      "199 bert.classifier.weight\n",
      "200 bert.classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for i, name_param in enumerate(bertclassifier.named_parameters()):\n",
    "    print(i, name_param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertclassifier.freeze_layers(100) # we want to train last 100 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2142, 0.2317, 0.2065, 0.1955, 0.1520]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertclassifier.eval()\n",
    "bertclassifier(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# checking the frozen layers\n",
    "print(list(bertclassifier.parameters())[-101].requires_grad)\n",
    "\n",
    "# trainable layer\n",
    "print(list(bertclassifier.parameters())[-100].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.8032, grad_fn=<NllLossBackward>),\n",
       " tensor([[ 0.2324,  0.2063,  0.4785, -0.0063,  0.0057]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertclassifier.train()\n",
    "bertclassifier(torch.tensor(encoded['input_ids']).unsqueeze(0),\n",
    "               torch.tensor(encoded['attention_mask']).unsqueeze(0), \n",
    "               torch.tensor([4]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# create our dataset\n",
    "\n",
    "class BBCArticleDataset(Dataset):\n",
    "    def __init__(self, texts, class_labels, class_id, tokenizer, train = True, train_split = 0.8): # pass the text and the labels, class_id : dict -> ohe\n",
    "        \n",
    "        self.train = train\n",
    "        \n",
    "        assert(len(class_labels) == len(texts))\n",
    "        assert(type(class_id) == dict)\n",
    "        \n",
    "        if self.train:\n",
    "            self.texts = texts[: int(len(texts)*train_split) ]\n",
    "            self.class_labels = class_labels[: int(len(texts)*train_split) ]\n",
    "        else:\n",
    "            self.texts = texts[ int(len(texts)*train_split): ]\n",
    "            self.class_labels = class_labels[ int(len(texts)*train_split): ]\n",
    "        self.class_id = class_id\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.class_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        encoded = tokenizer.encode_plus(self.texts[idx], add_special_tokens=True, max_length = 512, pad_to_max_length = True,\n",
    "                                return_token_type_ids = False,\n",
    "                                return_attention_mask = True)  # Batch size 1\n",
    "            \n",
    "        # text to tokens\n",
    "        token = torch.tensor(encoded['input_ids']) #.unsqueeze(0)\n",
    "        mask = torch.tensor(encoded['attention_mask'])\n",
    "        # ohe label\n",
    "        ohe = torch.tensor(class_id[ class_labels[idx] ])\n",
    "        \n",
    "        if self.train:\n",
    "            return token, mask, ohe\n",
    "        else:\n",
    "            return token, ohe # no mask needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = {'business' : 0, 'entertainment' : 1,\n",
    "           'politics' : 2, 'sport' : 3,\n",
    "           'tech' : 4}\n",
    "\n",
    "bbc_dataset_train = BBCArticleDataset(texts, class_labels, class_id, tokenizer)\n",
    "bbc_dataset_test = BBCArticleDataset(texts, class_labels, class_id, tokenizer, train = False)\n",
    "\n",
    "bbc_dataloader_train = DataLoader(bbc_dataset_train, batch_size=4, shuffle=True)\n",
    "bbc_dataloader_test = DataLoader(bbc_dataset_test, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(bbc_dataset_train[0][0].shape)\n",
    "print(bbc_dataset_train[0][1].shape)\n",
    "print(bbc_dataset_train[0][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "1\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "2\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "3\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "4\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "5\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "6\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "7\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "8\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "9\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "10\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# let's look at few examples\n",
    "\n",
    "for i_batch, sample_batch in enumerate(bbc_dataloader_train):\n",
    "    print(i_batch)\n",
    "    X, mask_X, y = sample_batch\n",
    "    print(X.shape)\n",
    "    print(mask_X.shape)\n",
    "    print(y.shape)\n",
    "    if i_batch == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23077e75b9a543779128297876bb26a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:35: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512da38c8c544629bc0f6bf2345c02ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.811151514294442\n",
      "epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c681dda95f4981bdd3bfc3e03a01e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7604973431383626\n",
      "epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57399dd285b04317a86d09b4931d03b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.8065094112010485\n",
      "epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c9b76bede146fca7c427a266a66ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7975520553883542\n",
      "epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ec40d24d0f45e2ab579736241ba0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.823687378342232\n",
      "epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6126dabcf64d719c7aee3f574839e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7756558592399854\n",
      "epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d6ef36e4b244d388c545bc6ac4c223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7749221283398318\n",
      "epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265961e6c8a8426cb058b1d3eab2df41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7657160914346073\n",
      "epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2d06e02bbc458da273989484259485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7464462553517202\n",
      "epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7edf4ddbe34eab86d8e9554de0282a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7588181217065018\n",
      "epoch: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc362abfb0b4ba6a1433d568e05e1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.764433454127794\n",
      "epoch: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0d6e9efad741d1bd68941410fac4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.779360019758846\n",
      "epoch: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec71673e39104ec1a17b9be80f9b980f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7568947082155206\n",
      "epoch: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e05a95ce5848859314438136cfc3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7636091849777136\n",
      "epoch: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6145dfa124cb4608ae8962e7db700eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.7338433683588264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # cuda for gpu acceleration\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(bertclassifier.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "bertclassifier.to(device) # taking the model to gpu if possible\n",
    "\n",
    "# metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "train_metrics = {'acc': [], 'f1': []}\n",
    "test_metrics = {'acc': [], 'f1': []}\n",
    "\n",
    "# progress bar\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "for e in tqdm_notebook(range(epochs)):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_f1 = 0.0\n",
    "    batch_cnt = 0\n",
    "    \n",
    "    bertclassifier.train()\n",
    "    \n",
    "    print(f'epoch: {e+1}')\n",
    "    \n",
    "    for i_batch, (X, X_mask, y) in tqdm_notebook(enumerate(bbc_dataloader_train)):\n",
    "        X = X.to(device)\n",
    "        X_mask = X_mask.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, y_pred = bertclassifier(X, X_mask, y)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = torch.argmax(y_pred, dim = -1)\n",
    "        \n",
    "        # update metrics\n",
    "        train_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "        train_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'micro')\n",
    "        batch_cnt += 1\n",
    "    \n",
    "    print(f'train loss: {train_loss/batch_cnt}')\n",
    "    train_losses.append(train_loss/batch_cnt)\n",
    "    train_metrics['acc'].append(train_acc/batch_cnt)\n",
    "    train_metrics['f1'].append(train_f1/batch_cnt)\n",
    "        \n",
    "        \n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_f1 = 0.0\n",
    "    batch_cnt = 0\n",
    "    \n",
    "    bertclassifier.eval()\n",
    "    with torch.no_grad():\n",
    "        for i_batch, (X, y) in enumerate(bbc_dataloader_test):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = bertclassifier(X) # in eval model we get the softmax output so, don't need to index\n",
    "\n",
    "            \n",
    "            y_pred = torch.argmax(y_pred, dim = -1)\n",
    "\n",
    "            # update metrics\n",
    "            test_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "            test_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'micro')\n",
    "            batch_cnt += 1\n",
    "            \n",
    "    test_metrics['acc'].append(test_acc/batch_cnt)\n",
    "    test_metrics['f1'].append(test_f1/batch_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.19719101123595506,\n",
       "  0.2151685393258427,\n",
       "  0.21123595505617979,\n",
       "  0.20674157303370785,\n",
       "  0.19325842696629214,\n",
       "  0.21179775280898877,\n",
       "  0.20337078651685395,\n",
       "  0.20561797752808988,\n",
       "  0.20168539325842696,\n",
       "  0.2095505617977528,\n",
       "  0.20337078651685395,\n",
       "  0.20280898876404493,\n",
       "  0.21629213483146068,\n",
       "  0.20842696629213484,\n",
       "  0.2140449438202247],\n",
       " 'f1': [0.19719101123595506,\n",
       "  0.2151685393258427,\n",
       "  0.21123595505617979,\n",
       "  0.20674157303370785,\n",
       "  0.19325842696629214,\n",
       "  0.21179775280898877,\n",
       "  0.20337078651685395,\n",
       "  0.20561797752808988,\n",
       "  0.20168539325842696,\n",
       "  0.2095505617977528,\n",
       "  0.20337078651685395,\n",
       "  0.20280898876404493,\n",
       "  0.21629213483146068,\n",
       "  0.20842696629213484,\n",
       "  0.2140449438202247]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.24330357142857142,\n",
       "  0.16517857142857142,\n",
       "  0.2700892857142857,\n",
       "  0.16294642857142858,\n",
       "  0.16517857142857142,\n",
       "  0.23660714285714285,\n",
       "  0.24330357142857142,\n",
       "  0.26339285714285715,\n",
       "  0.16517857142857142,\n",
       "  0.26339285714285715,\n",
       "  0.26339285714285715,\n",
       "  0.23660714285714285,\n",
       "  0.16517857142857142,\n",
       "  0.16517857142857142,\n",
       "  0.16294642857142858],\n",
       " 'f1': [0.24330357142857142,\n",
       "  0.16517857142857142,\n",
       "  0.2700892857142857,\n",
       "  0.16294642857142858,\n",
       "  0.16517857142857142,\n",
       "  0.23660714285714285,\n",
       "  0.24330357142857142,\n",
       "  0.26339285714285715,\n",
       "  0.16517857142857142,\n",
       "  0.26339285714285715,\n",
       "  0.26339285714285715,\n",
       "  0.23660714285714285,\n",
       "  0.16517857142857142,\n",
       "  0.16517857142857142,\n",
       "  0.16294642857142858]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "for name, param in bertclassifier.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(a, b, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
